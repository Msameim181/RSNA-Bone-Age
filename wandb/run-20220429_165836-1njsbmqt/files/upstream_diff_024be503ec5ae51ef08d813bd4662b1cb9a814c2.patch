diff --git a/Run.py b/Run.py
index d24aee6..f06c1ca 100644
--- a/Run.py
+++ b/Run.py
@@ -13,10 +13,27 @@ from models.ResNet import ResNet18, ResNet34, ResNet50, ResNet101, ResNet152
 # Custom libs
 from Train import trainer
 from utils.dataloader import RSNATestDataset, RSNATrainDataset, data_wrapper
+from utils.get_args import get_args
 
+def select_model(args, image_channels, num_classes):
+    if args.model == 'ResNet18':
+        return ResNet18(pretrained = args.pretrained, image_channels = 1, num_classes = num_classes)
+    elif args.model == 'ResNet34':
+        return ResNet34(pretrained = args.pretrained, image_channels = 1, num_classes = num_classes)
+    elif args.model == 'ResNet50':
+        return ResNet50(pretrained = args.pretrained, image_channels = 1, num_classes = num_classes)
+    elif args.model == 'ResNet101':
+        return ResNet101(pretrained = args.pretrained, image_channels = 1, num_classes = num_classes)
+    elif args.model == 'ResNet152':
+        return ResNet152(pretrained = args.pretrained, image_channels = 1, num_classes = num_classes)
+    elif args.model == 'MobileNet_V2':
+        return MobileNet_V2(pretrained = args.pretrained, image_channels = 1, num_classes = num_classes)
+    else:
+        assert None, 'Model not supported.'
 
 if __name__ == '__main__':
-
+    # Get args
+    args = get_args()
     # Set up logger
     logging.basicConfig(level = logging.INFO, format = '%(levelname)s: %(message)s')
     logging.info('Starting program...')
@@ -38,14 +55,15 @@ if __name__ == '__main__':
                            image_dir = Path(defualt_path, 'dataset/rsna-bone-age/boneage-training-dataset/boneage-training-dataset/'),
                            basedOnSex = basedOnSex, gender = gender)
 
-    test_dataset = RSNATestDataset(data_file = defualt_path + 'dataset/rsna-bone-age/boneage-test-dataset.csv',
-                           image_dir = defualt_path + 'dataset/rsna-bone-age/boneage-test-dataset/boneage-test-dataset/',
+    test_dataset = RSNATestDataset(data_file = Path(defualt_path, 'dataset/rsna-bone-age/boneage-test-dataset.csv'),
+                           image_dir = Path(defualt_path, 'dataset/rsna-bone-age/boneage-test-dataset/boneage-test-dataset/'),
                            basedOnSex = basedOnSex, gender = gender)
     num_classes = train_dataset.num_classes
 
     # Loading NN model
     logging.info('Loading NN Model...')
-    net = MobileNet_V2(pretrained = True, image_channels=1, num_classes=num_classes)
+    # net = ResNet18(pretrained = True, image_channels=1, num_classes=num_classes)
+    net = select_model(args = args, image_channels=1, num_classes=num_classes)
     logging.info(f'Model loaded as "{net.name}"')
     logging.info(f'Network:\n'
                  f'\t{net.in_channels} input channels\n'
@@ -53,11 +71,11 @@ if __name__ == '__main__':
 
     # Set up training hyperparameters
     logging.info('Reading hyperparameters...')
-    learning_rate = 0.001
-    epochs = 10
-    batch_size = 2
-    val_percent = 0.3
-    WandB_usage = True
+    learning_rate = args.learning_rate
+    epochs = args.epochs
+    batch_size = args.batch_size
+    val_percent = args.val_percent
+    WandB_usage = args.wandb
 
     # Packaging the data
     logging.info('Packaging the data...')
@@ -92,8 +110,8 @@ if __name__ == '__main__':
         batch_size = batch_size, 
         learning_rate = learning_rate, 
         val_percent = val_percent,
-        amp = False, 
-        save_checkpoint = True, 
+        amp = args.amp, 
+        save_checkpoint = args.checkpoint, 
         dir_checkpoint = './checkpoints/',
         run_name = run_name,
         WandB_usage = WandB_usage)
diff --git a/Temp.py b/Temp.py
index 7d6e2e5..7270aa7 100644
--- a/Temp.py
+++ b/Temp.py
@@ -68,114 +68,97 @@
 #     # train_net(net, device, train_loader, test_loader, 
 #     #         epochs, batch_size, learning_rate)
 
+import argparse
+# def get_args():
+#     parser = argparse.ArgumentParser(description='Train the UNet on images and target masks')
+#     parser.add_argument('--epochs', '-e', metavar='E', type=int, default=50, help='Number of epochs')
+#     parser.add_argument('--batch-size', '-b', dest='batch_size', metavar='B', type=int, default=1, help='Batch size')
+#     parser.add_argument('--learning-rate', '-l', metavar='LR', type=float, default=0.00001,
+#                         help='Learning rate', dest='lr')
+#     parser.add_argument('--load', '-f', type=str, default=False, help='Load model from a .pth file')
+#     parser.add_argument('--scale', '-s', type=float, default=0.5, help='Downscaling factor of the images')
+#     parser.add_argument('--validation', '-v', dest='val', type=float, default=10.0,
+#                         help='Percent of the data that is used as validation (0-100)')
+#     parser.add_argument('--amp', action='store_true', default=False, help='Use mixed precision')
 
+#     return parser.parse_args()
 
-# global_step = 4414
-# n_train = 8828
-# batch_size = 2
-# n = 2
-# es = 0
-# # n_train = n_train // batch_size
-# # # point = [0 if item == n else (n_train//n) * item  for item in range(1, n + 1)]
-# # co = 0
-# # print(point)
-# for i in range(n_train // batch_size):
-#     es += 1
-#     global_step += 1
-#     epoch_step = (global_step % (n_train // batch_size)) if global_step > (n_train // batch_size) else global_step
-# #     epoch_step = (global_step % n_train) if global_step >= n_train else global_step
-# #     if epoch_step in point:
-# #         print(global_step*batch_size)
-# #         co += 1
-#     print(epoch_step, es)
-# # print(co)
-
-
-import logging
-from pathlib import Path
-
-import numpy as np
-from tensorboardX import SummaryWriter
-from models.MobileNet import MobileNet_V2
-import torch
-
-def tb_setup(config, log_dir:str = './tensorboard'):
-    """
-    Setup tensorboard logger
-    """
-    if not log_dir:
-        log_dir = './tensorboard'
-    net = config['net']
-    model = config['model']
-    name = config['name']
-    device = config['device']
-    epochs = config['epochs']
-    batch_size = config['batch_size']
-    learning_rate = config['learning_rate']
-    save_checkpoint = config['save_checkpoint']
-    amp = config['amp']
-
-    # Create a run
-    tb_logger = SummaryWriter(log_dir=Path(log_dir, name))
-
-    tb_logger.add_text(tag='name', text_string=str(name), global_step=0)
-    tb_logger.add_text(tag='model', text_string=str(model), global_step=0)
-    tb_logger.add_text(tag='device', text_string=str(device), global_step=0)
-    tb_logger.add_text(tag='epochs', text_string=str(epochs), global_step=0)
-    tb_logger.add_text(tag='batch_size', text_string=str(batch_size), global_step=0)
-    tb_logger.add_text(tag='learning_rate', text_string=str(learning_rate), global_step=0)
-    tb_logger.add_text(tag='amp', text_string=str(amp), global_step=0)
-    tb_logger.add_text(tag='save_checkpoint', text_string=str(save_checkpoint), global_step=0)
-
-    tb_logger.add_graph(net.cuda(), ([torch.randn(batch_size, 1, 500, 625).cuda(), torch.randn(batch_size).cuda()], ))
-    return tb_logger
-
-
-def tb_log_training_step(tb_logger, loss, global_step, epoch, epoch_loss_step):
-     # Logging
-    tb_logger.add_scalar('Loss/Step Loss', loss.item(), global_step)
-    tb_logger.add_scalar('Loss/Train Loss (Step)', epoch_loss_step, global_step)
-    tb_logger.add_scalar('Process/Step', global_step, global_step)
-    tb_logger.add_scalar('Process/Epoch', epoch, global_step)
-
-
-def tb_log_training(tb_logger, epoch_loss, val_loss, epoch):
-    # Logging
-    tb_logger.add_scalar('Loss/Validation Loss (Epoch)', val_loss, epoch)
-    tb_logger.add_scalar('Loss/Train Loss', epoch_loss, epoch)
-    tb_logger.add_scalar('Loss/Epoch Loss', epoch_loss, epoch)
-    logging.info(f'\nEpoch: {epoch} | Train Loss: {epoch_loss:.4f} | Validation Loss: {val_loss:.4f}\n')
+
+
+def get_args():
+    parser = argparse.ArgumentParser(description='Train the Your Model on images and target age.')
+
+    parser.add_argument('--epochs', '-e', metavar='E', type=int, default=10, help='Number of epochs')
     
+    parser.add_argument('--model-type', '-m', dest='model', metavar='M', type=str, 
+                        default="ResNet18", help='The name of the model to use')
+
+    parser.add_argument('--pretrained', '-p', dest='pretrained', metavar='P', type=bool, 
+                        default=True, help='Using pretrained model')
+
+    parser.add_argument('--wandb', '-w', dest='wandb', metavar='W', type=bool, 
+                        default=True, help='Using WandB')
 
+    parser.add_argument('--checkpoint', '-c', dest='wandb', metavar='W', type=bool, 
+                        default=True, help='Saving checkpoints')
 
-def tb_log_validation(tb_logger, optimizer, val_loss, acc, 
-    images, batch_size, global_step, epoch, net):
-    # TensorBoard Storing the results
-    tb_logger.add_scalar('Process/Learning Rate', optimizer.param_groups[0]['lr'], global_step)
-    tb_logger.add_scalar('Loss/Validation Loss (Step)', val_loss, global_step)
-    tb_logger.add_scalar('Accuracy/Validation Correct (Step)', acc, global_step)
-    tb_logger.add_scalar('Accuracy/Correct %', acc * 100, global_step)
-    tb_logger.add_scalar('Process/Step', global_step, global_step)
-    tb_logger.add_scalar('Process/Epoch', epoch, global_step)
-    # img_batch = images.cpu() if batch_size == 1 else [image.cpu() for image in images]
-    # tb_logger.add_images('Data/Images', img_batch, global_step)
+    parser.add_argument('--batch-size', '-b', dest='batch_size', metavar='B', type=int, default=1, help='Batch size')
 
-    for name, param in net.named_parameters():
-        tb_logger.add_histogram(name, param.clone().cpu().data.numpy(), global_step)
+    parser.add_argument('--learning-rate', '-l', metavar='LR', type=float, default=0.0001,
+                        help='Learning rate', dest='learning_rate')
 
+    parser.add_argument('--load', '-f', type=str, default=False, help='Load model from a .pth file')
+
+    parser.add_argument('--validation', '-v', dest='val_percent', type=float, default=10.0,
+                        help='Percent of the data that is used as validation (0-100)')
+
+    parser.add_argument('--amp', action='store_true', default=False, help='Use mixed precision')
+
+    return parser.parse_args()
 
 
 if __name__ == '__main__':
     # Test
-    tb_setup(dict(
-            net = MobileNet_V2(pretrained = True, image_channels = 1, num_classes = 229), 
-            epochs = 1000, 
-            batch_size = 1, 
-            learning_rate = 0.001,
-            save_checkpoint = './checkpoints/', 
-            amp = False,
-            model = "net.name",
-            name = "run_namessss",
-            device = "devicessss",
-            optimizer = "optimizer.__class__.__name__",
-            criterion = "criterion.__class__.__name__"))
+    arg = get_args()
+    print(arg.model)
+
+# def packaging(batch_size, n_train):
+#     batch_num = n_train // batch_size
+#     if n_train % batch_size != 0:
+#         batch_num += 1
+#     return batch_num
+
+# def vel_section(batch_size, n_train, val_repeat, global_step):
+#     n_train_batch = n_train // batch_size
+#     val_point = [0 if item == val_repeat else ((n_train_batch//val_repeat) * item)  for item in range(1, val_repeat + 1)]
+#     n_train_batch += 1
+#     epoch_step = (global_step % n_train_batch) if global_step >= n_train_batch else global_step
+#     if epoch_step in val_point:
+#         print('val_point(epoch_step):', epoch_step)
+#         print('val_point(global_step): ', global_step)
+#         return True
+# if __name__ == '__main__':
+#     # Test
+
+
+
+#     val_repeat = 2
+#     batch_size = 6
+#     n_train = 8828
+#     global_step = 0
+
+
+#     print(f"n_train // batch_size: {n_train // batch_size}")
+#     print(f"n_train % batch_size: {n_train % batch_size}")
+#     print(packaging(batch_size=batch_size, n_train=n_train))
+#     # print(8828 % 2)
+#     for epoch in range(1):
+#         print(f"epoch: {epoch}")
+#         epoch_step = 0
+#         for i in range(packaging(batch_size, n_train)):
+#             global_step += 1
+#             epoch_step += 1
+#             if ch := vel_section(batch_size, n_train, val_repeat, global_step):
+#                 log_epoch_loss = ((((epoch_step - 1) * batch_size) + (n_train % batch_size))) if n_train // batch_size == (epoch_step - 1) else ((epoch_step * batch_size))
+#                 print(f"val_loss T {epoch_step} / {epoch_step * batch_size} / {global_step} / {log_epoch_loss}")
+#             # print(i)
\ No newline at end of file
diff --git a/Train.py b/Train.py
index 48cd40e..c4921b2 100644
--- a/Train.py
+++ b/Train.py
@@ -136,20 +136,21 @@ def trainer(
                 global_step += 1
                 epoch_step += 1
                 epoch_loss += loss.item()
-                
+
+                # Logging
+                log_epoch_loss = (epoch_loss / (((epoch_step - 1) * batch_size) + (n_train % batch_size))) if n_train // batch_size == (epoch_step - 1) else (epoch_loss / (epoch_step * batch_size))
                 # Update the progress bar
-                pbar.set_postfix(**{'Step Loss (Batch)': loss.item(), 'Epoch Loss (Train)': epoch_loss / (epoch_step * batch_size)})
+                pbar.set_postfix(**{'Step Loss (Batch)': loss.item(), 'Epoch Loss (Train)': log_epoch_loss})
                 # Logging
                 if WandB_usage:
-                    wandb_log_training_step(wandb_logger, loss, global_step, epoch, epoch_loss / (epoch_step * batch_size))
+                    wandb_log_training_step(wandb_logger, loss, global_step, epoch, log_epoch_loss)
                 
-                tb_log_training_step(tb_logger, loss, global_step, epoch, epoch_loss / (epoch_step * batch_size))
+                tb_log_training_step(tb_logger, loss, global_step, epoch, log_epoch_loss)
                 
                 # Validation
                 val_loss = validation(wandb_logger, tb_logger, net, device, optimizer, scheduler, criterion, 
                     epoch, global_step, epoch_step, n_train, batch_size,val_loader, images, 
                     boneage, age_pred, gender, WandB_usage)
-
                 net.train()
 
         # Logging
@@ -190,14 +191,14 @@ def validation(
     val_repeat:int = 2) -> None:
     """Validation Worker
     """
-    val_loss = None
+
     # Evaluation round
     # Let's See if is it evaluation time or not
     n_train_batch = n_train // batch_size
     val_point = [0 if item == val_repeat else ((n_train_batch//val_repeat) * item)  for item in range(1, val_repeat + 1)]
+    n_train_batch += 1
     epoch_step = (global_step % n_train_batch) if global_step >= n_train_batch else global_step
     if epoch_step in val_point:
-        logging.info(f'Val accept on {global_step}')
 
         # WandB Storing the model parameters
         if WandB_usage:
@@ -220,5 +221,6 @@ def validation(
 
         logging.info('Validation completed.')
         logging.info('Result Saved.')
+        return val_loss
 
-    return val_loss
+    return None
diff --git a/Validation.py b/Validation.py
index b23daf6..aa17106 100644
--- a/Validation.py
+++ b/Validation.py
@@ -39,6 +39,7 @@ def validate(net, val_loader, device, criterion):
         acc /= n_val
 
     # Logging
+    print("\n")
     logging.info(f'\nValidation set:\n'
                  f'\tAverage loss: {val_loss:.4f}'
                  f'\tAccuracy: {acc * 100:.2f}%\tCorrect = {correct}/{n_val}\n')
diff --git a/__pycache__/Train.cpython-39.pyc b/__pycache__/Train.cpython-39.pyc
index 056dcf4..c36cba5 100644
Binary files a/__pycache__/Train.cpython-39.pyc and b/__pycache__/Train.cpython-39.pyc differ
diff --git a/__pycache__/Validation.cpython-39.pyc b/__pycache__/Validation.cpython-39.pyc
index e371d5c..73e3cfd 100644
Binary files a/__pycache__/Validation.cpython-39.pyc and b/__pycache__/Validation.cpython-39.pyc differ
diff --git a/checkpoints/checkpoint_epoch1.pth b/checkpoints/checkpoint_epoch1.pth
index 7bd9835..9da00be 100644
Binary files a/checkpoints/checkpoint_epoch1.pth and b/checkpoints/checkpoint_epoch1.pth differ
diff --git a/checkpoints/checkpoint_epoch2.pth b/checkpoints/checkpoint_epoch2.pth
index 38f1c84..5222cd2 100644
Binary files a/checkpoints/checkpoint_epoch2.pth and b/checkpoints/checkpoint_epoch2.pth differ
diff --git a/tensorboard/20220425_174225_ResNet34_Pre2/events.out.tfevents.1650892352.Mahdi-Omen.14220.0 b/tensorboard/20220425_174225_ResNet34_Pre2/events.out.tfevents.1650892352.Mahdi-Omen.14220.0
new file mode 100644
index 0000000..e16aeee
Binary files /dev/null and b/tensorboard/20220425_174225_ResNet34_Pre2/events.out.tfevents.1650892352.Mahdi-Omen.14220.0 differ
diff --git a/tensorboard/20220425_193310_ResNet50_Pre2/events.out.tfevents.1650899000.Mahdi-Omen.14748.0 b/tensorboard/20220425_193310_ResNet50_Pre2/events.out.tfevents.1650899000.Mahdi-Omen.14748.0
new file mode 100644
index 0000000..663ed86
Binary files /dev/null and b/tensorboard/20220425_193310_ResNet50_Pre2/events.out.tfevents.1650899000.Mahdi-Omen.14748.0 differ
diff --git a/utils/__pycache__/tensorboard_logger.cpython-39.pyc b/utils/__pycache__/tensorboard_logger.cpython-39.pyc
index c41c90c..1ff041f 100644
Binary files a/utils/__pycache__/tensorboard_logger.cpython-39.pyc and b/utils/__pycache__/tensorboard_logger.cpython-39.pyc differ
diff --git a/utils/__pycache__/wandb_logger.cpython-39.pyc b/utils/__pycache__/wandb_logger.cpython-39.pyc
index 882779e..80717e5 100644
Binary files a/utils/__pycache__/wandb_logger.cpython-39.pyc and b/utils/__pycache__/wandb_logger.cpython-39.pyc differ
diff --git a/utils/tensorboard_logger.py b/utils/tensorboard_logger.py
index eb1acce..d827957 100644
--- a/utils/tensorboard_logger.py
+++ b/utils/tensorboard_logger.py
@@ -3,6 +3,7 @@ from pathlib import Path
 
 import torch
 from torch.utils.tensorboard import SummaryWriter
+# from tensorboardX import SummaryWriter
 
 def tb_setup(config, log_dir:str = './tensorboard/'):
     """
@@ -52,10 +53,11 @@ def tb_log_training_step(tb_logger, loss, global_step, epoch, epoch_loss_step):
 
 def tb_log_training(tb_logger, epoch_loss, val_loss, epoch):
     # Logging
-    tb_logger.add_scalar('Loss/Validation Loss (Epoch)', val_loss, epoch)
+    print(f"val_loss TB1: {val_loss}")
     tb_logger.add_scalar('Loss/Train Loss', epoch_loss, epoch)
     tb_logger.add_scalar('Loss/Epoch Loss', epoch_loss, epoch)
-    logging.info(f'\nEpoch: {epoch} | Train Loss: {epoch_loss:.4f} | Validation Loss: {val_loss:.4f}\n')
+    tb_logger.add_scalar('Loss/Validation Loss (Epoch)', val_loss, epoch)
+    logging.info(f'\nEpoch: {epoch + 1} | Train Loss: {epoch_loss:.4f} | Validation Loss: {val_loss:.4f}\n')
 
     tb_logger.flush()
     
diff --git a/utils/wandb_logger.py b/utils/wandb_logger.py
index 313d393..04312e6 100644
--- a/utils/wandb_logger.py
+++ b/utils/wandb_logger.py
@@ -27,7 +27,7 @@ def wandb_setup(config) -> wandb:
             f'{device}'
         ],)
     
-    # wandb.tensorboard.patch(root_logdir="./tensorboard", tensorboardX=True)
+    # wandb.tensorboard.patch(root_logdir="./tensorboard", tensorboard_x=False)
     # Configure wandb
     wandb_logger.config.update(config)
     # Logging
@@ -38,9 +38,9 @@ def wandb_setup(config) -> wandb:
 def wandb_log_training_step(wandb_logger, loss, global_step, epoch, epoch_loss_step):
     # Logging
     wandb_logger.log({
-        'Loss/Step_Loss':            loss.item(),
+        'Loss/Step Loss':            loss.item(),
         'Process/Step':                 global_step,
-        'Loss/Train_Loss_(Step)':    epoch_loss_step,
+        'Loss/Train Loss (Step)':    epoch_loss_step,
         'Process/Epoch':                epoch
     })
 
