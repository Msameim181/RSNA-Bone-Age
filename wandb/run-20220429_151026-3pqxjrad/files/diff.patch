diff --git a/Run.py b/Run.py
index 40e19ed..f22ba3c 100644
--- a/Run.py
+++ b/Run.py
@@ -1,4 +1,5 @@
 # System and utils for preprocessing
+import argparse
 import logging
 import sys
 from datetime import datetime
@@ -15,6 +16,7 @@ from Train import trainer
 from utils.dataloader import RSNATestDataset, RSNATrainDataset, data_wrapper
 
 
+
 if __name__ == '__main__':
 
     # Set up logger
@@ -45,7 +47,7 @@ if __name__ == '__main__':
 
     # Loading NN model
     logging.info('Loading NN Model...')
-    net = ResNet50(pretrained = True, image_channels=1, num_classes=num_classes)
+    net = ResNet18(pretrained = True, image_channels=1, num_classes=num_classes)
     logging.info(f'Model loaded as "{net.name}"')
     logging.info(f'Network:\n'
                  f'\t{net.in_channels} input channels\n'
@@ -55,7 +57,7 @@ if __name__ == '__main__':
     logging.info('Reading hyperparameters...')
     learning_rate = 0.001
     epochs = 10
-    batch_size = 2
+    batch_size = 6
     val_percent = 0.3
     WandB_usage = True
 
diff --git a/Temp.py b/Temp.py
index 64fc67c..f44d7b7 100644
--- a/Temp.py
+++ b/Temp.py
@@ -69,21 +69,45 @@
 #     #         epochs, batch_size, learning_rate)
 
 import argparse
+# def get_args():
+#     parser = argparse.ArgumentParser(description='Train the UNet on images and target masks')
+#     parser.add_argument('--epochs', '-e', metavar='E', type=int, default=50, help='Number of epochs')
+#     parser.add_argument('--batch-size', '-b', dest='batch_size', metavar='B', type=int, default=1, help='Batch size')
+#     parser.add_argument('--learning-rate', '-l', metavar='LR', type=float, default=0.00001,
+#                         help='Learning rate', dest='lr')
+#     parser.add_argument('--load', '-f', type=str, default=False, help='Load model from a .pth file')
+#     parser.add_argument('--scale', '-s', type=float, default=0.5, help='Downscaling factor of the images')
+#     parser.add_argument('--validation', '-v', dest='val', type=float, default=10.0,
+#                         help='Percent of the data that is used as validation (0-100)')
+#     parser.add_argument('--amp', action='store_true', default=False, help='Use mixed precision')
+
+#     return parser.parse_args()
+
+if __name__ == '__main__':
+    # Test
+    arg = get_args()
+    print(arg)
+
+
+
 def get_args():
-    parser = argparse.ArgumentParser(description='Train the UNet on images and target masks')
-    parser.add_argument('--epochs', '-e', metavar='E', type=int, default=50, help='Number of epochs')
+    parser = argparse.ArgumentParser(description='Train the Your Model on images and target age.')
+
+    parser.add_argument('--epochs', '-e', metavar='E', type=int, default=10, help='Number of epochs')
+    
+    parser.add_argument('--model-type', '-m', dest='model', metavar='M', type=str, 
+                        default="ResNet18", help='The name of the model to use')
+
     parser.add_argument('--batch-size', '-b', dest='batch_size', metavar='B', type=int, default=1, help='Batch size')
-    parser.add_argument('--learning-rate', '-l', metavar='LR', type=float, default=0.00001,
-                        help='Learning rate', dest='lr')
+
+    parser.add_argument('--learning-rate', '-l', metavar='LR', type=float, default=0.0001,
+                        help='Learning rate', dest='learning_rate')
+
     parser.add_argument('--load', '-f', type=str, default=False, help='Load model from a .pth file')
-    parser.add_argument('--scale', '-s', type=float, default=0.5, help='Downscaling factor of the images')
-    parser.add_argument('--validation', '-v', dest='val', type=float, default=10.0,
+
+    parser.add_argument('--validation', '-v', dest='val_percent', type=float, default=10.0,
                         help='Percent of the data that is used as validation (0-100)')
+
     parser.add_argument('--amp', action='store_true', default=False, help='Use mixed precision')
 
     return parser.parse_args()
-
-if __name__ == '__main__':
-    # Test
-    arg = get_args()
-    print(arg)
\ No newline at end of file
diff --git a/Train.py b/Train.py
index a848375..a01ed9f 100644
--- a/Train.py
+++ b/Train.py
@@ -149,13 +149,14 @@ def trainer(
                 val_loss = validation(wandb_logger, tb_logger, net, device, optimizer, scheduler, criterion, 
                     epoch, global_step, epoch_step, n_train, batch_size,val_loader, images, 
                     boneage, age_pred, gender, WandB_usage)
-
+                print(f"val_loss T {epoch_step * batch_size} / {global_step}: {val_loss}")
                 net.train()
 
         # Logging
         if WandB_usage:
             wandb_log_training(wandb_logger, epoch_loss / n_train, val_loss, epoch)
 
+        print(f"val_loss T2 {epoch_step * batch_size} / {global_step}: {val_loss}")
         tb_log_training(tb_logger, epoch_loss / n_train, val_loss, epoch)
 
         # Save the model checkpoint
@@ -190,7 +191,7 @@ def validation(
     val_repeat:int = 2) -> None:
     """Validation Worker
     """
-    val_loss = None
+
     # Evaluation round
     # Let's See if is it evaluation time or not
     n_train_batch = n_train // batch_size
@@ -219,5 +220,7 @@ def validation(
 
         logging.info('Validation completed.')
         logging.info('Result Saved.')
+        print(f"val_loss V {epoch_step} / {global_step} / {val_point}: {val_loss}")
+        return val_loss
 
-    return val_loss
+    return None
diff --git a/Validation.py b/Validation.py
index b23daf6..aa17106 100644
--- a/Validation.py
+++ b/Validation.py
@@ -39,6 +39,7 @@ def validate(net, val_loader, device, criterion):
         acc /= n_val
 
     # Logging
+    print("\n")
     logging.info(f'\nValidation set:\n'
                  f'\tAverage loss: {val_loss:.4f}'
                  f'\tAccuracy: {acc * 100:.2f}%\tCorrect = {correct}/{n_val}\n')
diff --git a/__pycache__/Train.cpython-39.pyc b/__pycache__/Train.cpython-39.pyc
index 789041a..ac2050d 100644
Binary files a/__pycache__/Train.cpython-39.pyc and b/__pycache__/Train.cpython-39.pyc differ
diff --git a/__pycache__/Validation.cpython-39.pyc b/__pycache__/Validation.cpython-39.pyc
index e371d5c..73e3cfd 100644
Binary files a/__pycache__/Validation.cpython-39.pyc and b/__pycache__/Validation.cpython-39.pyc differ
diff --git a/utils/__pycache__/tensorboard_logger.cpython-39.pyc b/utils/__pycache__/tensorboard_logger.cpython-39.pyc
index a0f5daa..7723979 100644
Binary files a/utils/__pycache__/tensorboard_logger.cpython-39.pyc and b/utils/__pycache__/tensorboard_logger.cpython-39.pyc differ
diff --git a/utils/__pycache__/wandb_logger.cpython-39.pyc b/utils/__pycache__/wandb_logger.cpython-39.pyc
index 882779e..80717e5 100644
Binary files a/utils/__pycache__/wandb_logger.cpython-39.pyc and b/utils/__pycache__/wandb_logger.cpython-39.pyc differ
diff --git a/utils/tensorboard_logger.py b/utils/tensorboard_logger.py
index 03fea7d..2228fc4 100644
--- a/utils/tensorboard_logger.py
+++ b/utils/tensorboard_logger.py
@@ -3,6 +3,7 @@ from pathlib import Path
 
 import torch
 from torch.utils.tensorboard import SummaryWriter
+# from tensorboardX import SummaryWriter
 
 def tb_setup(config, log_dir:str = './tensorboard/'):
     """
@@ -52,9 +53,14 @@ def tb_log_training_step(tb_logger, loss, global_step, epoch, epoch_loss_step):
 
 def tb_log_training(tb_logger, epoch_loss, val_loss, epoch):
     # Logging
-    tb_logger.add_scalar('Loss/Validation Loss (Epoch)', val_loss, epoch)
+    print(f"val_loss TB1: {val_loss}")
     tb_logger.add_scalar('Loss/Train Loss', epoch_loss, epoch)
     tb_logger.add_scalar('Loss/Epoch Loss', epoch_loss, epoch)
+    try:
+        tb_logger.add_scalar('Loss/Validation Loss (Epoch)', val_loss, epoch)
+    except Exception as e:
+        print(f"The Error: {e}")
+        print(f"val_loss TB2: {val_loss}")
     logging.info(f'\nEpoch: {epoch + 1} | Train Loss: {epoch_loss:.4f} | Validation Loss: {val_loss:.4f}\n')
 
     tb_logger.flush()
diff --git a/utils/wandb_logger.py b/utils/wandb_logger.py
index 313d393..04312e6 100644
--- a/utils/wandb_logger.py
+++ b/utils/wandb_logger.py
@@ -27,7 +27,7 @@ def wandb_setup(config) -> wandb:
             f'{device}'
         ],)
     
-    # wandb.tensorboard.patch(root_logdir="./tensorboard", tensorboardX=True)
+    # wandb.tensorboard.patch(root_logdir="./tensorboard", tensorboard_x=False)
     # Configure wandb
     wandb_logger.config.update(config)
     # Logging
@@ -38,9 +38,9 @@ def wandb_setup(config) -> wandb:
 def wandb_log_training_step(wandb_logger, loss, global_step, epoch, epoch_loss_step):
     # Logging
     wandb_logger.log({
-        'Loss/Step_Loss':            loss.item(),
+        'Loss/Step Loss':            loss.item(),
         'Process/Step':                 global_step,
-        'Loss/Train_Loss_(Step)':    epoch_loss_step,
+        'Loss/Train Loss (Step)':    epoch_loss_step,
         'Process/Epoch':                epoch
     })
 
