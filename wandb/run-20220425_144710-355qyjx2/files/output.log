INFO: WandB setup completed.
INFO: Training Settings:
        Device:             cuda
        Model:              MobileNetV2_Pre2
        Image Channel:      1
        Epochs:             10
        Batch Size:         2
        Learning Rate:      0.001
        Training Size:      8828
        validation Size:    3783
        validation %:       0.3
        Checkpoints:        True
        Mixed Precision:    False
        optimizer:          Adam
        criterion:          BCEWithLogitsLoss
INFO: Start training as "20220425_144707_MobileNetV2_Pre2" ...


































































































































































Epoch 1/10:  45%|████████████████████████████████████████████▏                                                      | 3936/8828 [05:32<06:52, 11.85img/s, Epoch Loss (Train)=0.0118, Step Loss (Batch)=0.0171]
Traceback (most recent call last):
  File "c:\Users\mahdi\Desktop\Github_Work\1\RSNA-Bone-Age\Run.py", line 86, in <module>
    trainer(
  File "c:\Users\mahdi\Desktop\Github_Work\1\RSNA-Bone-Age\Train.py", line 129, in trainer
    grad_scaler.step(optimizer)
  File "C:\Python\Python39\lib\site-packages\torch\cuda\amp\grad_scaler.py", line 310, in step
    return optimizer.step(*args, **kwargs)
  File "C:\Python\Python39\lib\site-packages\torch\optim\optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "C:\Python\Python39\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "C:\Python\Python39\lib\site-packages\torch\optim\adam.py", line 141, in step
    F.adam(params_with_grad,
  File "C:\Python\Python39\lib\site-packages\torch\optim\_functional.py", line 105, in adam
    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
KeyboardInterrupt