diff --git a/T.py b/T.py
index 03d2d8e..7d6e2e5 100644
--- a/T.py
+++ b/T.py
@@ -70,22 +70,112 @@
 
 
 
-global_step = 4414
-n_train = 8828
-batch_size = 2
-n = 2
-es = 0
-# n_train = n_train // batch_size
-# # point = [0 if item == n else (n_train//n) * item  for item in range(1, n + 1)]
-# co = 0
-# print(point)
-for i in range(n_train // batch_size):
-    es += 1
-    global_step += 1
-    epoch_step = (global_step % (n_train // batch_size)) if global_step > (n_train // batch_size) else global_step
-#     epoch_step = (global_step % n_train) if global_step >= n_train else global_step
-#     if epoch_step in point:
-#         print(global_step*batch_size)
-#         co += 1
-    print(epoch_step, es)
-# print(co)
\ No newline at end of file
+# global_step = 4414
+# n_train = 8828
+# batch_size = 2
+# n = 2
+# es = 0
+# # n_train = n_train // batch_size
+# # # point = [0 if item == n else (n_train//n) * item  for item in range(1, n + 1)]
+# # co = 0
+# # print(point)
+# for i in range(n_train // batch_size):
+#     es += 1
+#     global_step += 1
+#     epoch_step = (global_step % (n_train // batch_size)) if global_step > (n_train // batch_size) else global_step
+# #     epoch_step = (global_step % n_train) if global_step >= n_train else global_step
+# #     if epoch_step in point:
+# #         print(global_step*batch_size)
+# #         co += 1
+#     print(epoch_step, es)
+# # print(co)
+
+
+import logging
+from pathlib import Path
+
+import numpy as np
+from tensorboardX import SummaryWriter
+from models.MobileNet import MobileNet_V2
+import torch
+
+def tb_setup(config, log_dir:str = './tensorboard'):
+    """
+    Setup tensorboard logger
+    """
+    if not log_dir:
+        log_dir = './tensorboard'
+    net = config['net']
+    model = config['model']
+    name = config['name']
+    device = config['device']
+    epochs = config['epochs']
+    batch_size = config['batch_size']
+    learning_rate = config['learning_rate']
+    save_checkpoint = config['save_checkpoint']
+    amp = config['amp']
+
+    # Create a run
+    tb_logger = SummaryWriter(log_dir=Path(log_dir, name))
+
+    tb_logger.add_text(tag='name', text_string=str(name), global_step=0)
+    tb_logger.add_text(tag='model', text_string=str(model), global_step=0)
+    tb_logger.add_text(tag='device', text_string=str(device), global_step=0)
+    tb_logger.add_text(tag='epochs', text_string=str(epochs), global_step=0)
+    tb_logger.add_text(tag='batch_size', text_string=str(batch_size), global_step=0)
+    tb_logger.add_text(tag='learning_rate', text_string=str(learning_rate), global_step=0)
+    tb_logger.add_text(tag='amp', text_string=str(amp), global_step=0)
+    tb_logger.add_text(tag='save_checkpoint', text_string=str(save_checkpoint), global_step=0)
+
+    tb_logger.add_graph(net.cuda(), ([torch.randn(batch_size, 1, 500, 625).cuda(), torch.randn(batch_size).cuda()], ))
+    return tb_logger
+
+
+def tb_log_training_step(tb_logger, loss, global_step, epoch, epoch_loss_step):
+     # Logging
+    tb_logger.add_scalar('Loss/Step Loss', loss.item(), global_step)
+    tb_logger.add_scalar('Loss/Train Loss (Step)', epoch_loss_step, global_step)
+    tb_logger.add_scalar('Process/Step', global_step, global_step)
+    tb_logger.add_scalar('Process/Epoch', epoch, global_step)
+
+
+def tb_log_training(tb_logger, epoch_loss, val_loss, epoch):
+    # Logging
+    tb_logger.add_scalar('Loss/Validation Loss (Epoch)', val_loss, epoch)
+    tb_logger.add_scalar('Loss/Train Loss', epoch_loss, epoch)
+    tb_logger.add_scalar('Loss/Epoch Loss', epoch_loss, epoch)
+    logging.info(f'\nEpoch: {epoch} | Train Loss: {epoch_loss:.4f} | Validation Loss: {val_loss:.4f}\n')
+    
+
+
+def tb_log_validation(tb_logger, optimizer, val_loss, acc, 
+    images, batch_size, global_step, epoch, net):
+    # TensorBoard Storing the results
+    tb_logger.add_scalar('Process/Learning Rate', optimizer.param_groups[0]['lr'], global_step)
+    tb_logger.add_scalar('Loss/Validation Loss (Step)', val_loss, global_step)
+    tb_logger.add_scalar('Accuracy/Validation Correct (Step)', acc, global_step)
+    tb_logger.add_scalar('Accuracy/Correct %', acc * 100, global_step)
+    tb_logger.add_scalar('Process/Step', global_step, global_step)
+    tb_logger.add_scalar('Process/Epoch', epoch, global_step)
+    # img_batch = images.cpu() if batch_size == 1 else [image.cpu() for image in images]
+    # tb_logger.add_images('Data/Images', img_batch, global_step)
+
+    for name, param in net.named_parameters():
+        tb_logger.add_histogram(name, param.clone().cpu().data.numpy(), global_step)
+
+
+
+if __name__ == '__main__':
+    # Test
+    tb_setup(dict(
+            net = MobileNet_V2(pretrained = True, image_channels = 1, num_classes = 229), 
+            epochs = 1000, 
+            batch_size = 1, 
+            learning_rate = 0.001,
+            save_checkpoint = './checkpoints/', 
+            amp = False,
+            model = "net.name",
+            name = "run_namessss",
+            device = "devicessss",
+            optimizer = "optimizer.__class__.__name__",
+            criterion = "criterion.__class__.__name__"))
diff --git a/Train.py b/Train.py
index a658de3..48cd40e 100644
--- a/Train.py
+++ b/Train.py
@@ -61,6 +61,7 @@ def trainer(
 
     # Initiate WandB
     config = dict(
+        net = net,
         epochs = epochs, 
         batch_size = batch_size, 
         learning_rate = learning_rate,
@@ -162,6 +163,9 @@ def trainer(
             Path(dir_checkpoint).mkdir(parents = True, exist_ok = True)
             torch.save(net.state_dict(), str(f"{dir_checkpoint}/checkpoint_epoch{epoch + 1}.pth"))
 
+    logging.info(f'Finished Training Course. \n')
+    tb_logger.close()
+    logging.info(f'Shutting Down... \n')
 
 # Validation Worker
 def validation(
@@ -212,7 +216,7 @@ def validation(
                 global_step, epoch, histograms)
 
         tb_log_validation(tb_logger, optimizer, val_loss, acc, 
-            images, batch_size, global_step, epoch)
+            images, batch_size, global_step, epoch, net)
 
         logging.info('Validation completed.')
         logging.info('Result Saved.')
diff --git a/__pycache__/Train.cpython-39.pyc b/__pycache__/Train.cpython-39.pyc
index 98d9bdb..056dcf4 100644
Binary files a/__pycache__/Train.cpython-39.pyc and b/__pycache__/Train.cpython-39.pyc differ
diff --git a/tensorboard/20220425_044950_MobileNetV2_Pre2/events.out.tfevents.1650845993.Mahdi-Omen.9140.0 b/tensorboard/20220425_044950_MobileNetV2_Pre2/events.out.tfevents.1650845993.Mahdi-Omen.9140.0
deleted file mode 100644
index 09668fd..0000000
Binary files a/tensorboard/20220425_044950_MobileNetV2_Pre2/events.out.tfevents.1650845993.Mahdi-Omen.9140.0 and /dev/null differ
diff --git a/utils/__pycache__/tensorboard_logger.cpython-39.pyc b/utils/__pycache__/tensorboard_logger.cpython-39.pyc
index 758cd26..4e0a8f3 100644
Binary files a/utils/__pycache__/tensorboard_logger.cpython-39.pyc and b/utils/__pycache__/tensorboard_logger.cpython-39.pyc differ
diff --git a/utils/__pycache__/wandb_logger.cpython-39.pyc b/utils/__pycache__/wandb_logger.cpython-39.pyc
index 4cd466b..f63e946 100644
Binary files a/utils/__pycache__/wandb_logger.cpython-39.pyc and b/utils/__pycache__/wandb_logger.cpython-39.pyc differ
diff --git a/utils/tensorboard_logger.py b/utils/tensorboard_logger.py
index 07ca56e..a072d58 100644
--- a/utils/tensorboard_logger.py
+++ b/utils/tensorboard_logger.py
@@ -1,16 +1,16 @@
 import logging
 from pathlib import Path
 
-import numpy as np
+import torch
 from torch.utils.tensorboard import SummaryWriter
 
-
 def tb_setup(config, log_dir:str = './tensorboard'):
     """
     Setup tensorboard logger
     """
     if not log_dir:
         log_dir = './tensorboard'
+    net = config['net']
     model = config['model']
     name = config['name']
     device = config['device']
@@ -32,15 +32,22 @@ def tb_setup(config, log_dir:str = './tensorboard'):
     tb_logger.add_text(tag='amp', text_string=str(amp), global_step=0)
     tb_logger.add_text(tag='save_checkpoint', text_string=str(save_checkpoint), global_step=0)
 
+    tb_logger.add_graph(net.cuda(), ([torch.randn(batch_size, 1, 500, 625).cuda(), torch.randn(batch_size).cuda()], ))
+
+    
+
     return tb_logger
 
 
+
 def tb_log_training_step(tb_logger, loss, global_step, epoch, epoch_loss_step):
      # Logging
-    tb_logger.add_scalar('Loss/Step Loss', loss.item(), global_step)
-    tb_logger.add_scalar('Loss/Train Loss (Step)', epoch_loss_step, global_step)
-    tb_logger.add_scalar('Process/Step', global_step, global_step)
-    tb_logger.add_scalar('Process/Epoch', epoch, global_step)
+    tb_logger.add_scalar('tLoss/Step Loss', loss.item(), global_step)
+    tb_logger.add_scalar('tLoss/Train Loss (Step)', epoch_loss_step, global_step)
+    tb_logger.add_scalar('tProcess/Step', global_step, global_step)
+    tb_logger.add_scalar('tProcess/Epoch', epoch, global_step)
+
+    
 
 
 def tb_log_training(tb_logger, epoch_loss, val_loss, epoch):
@@ -49,11 +56,13 @@ def tb_log_training(tb_logger, epoch_loss, val_loss, epoch):
     tb_logger.add_scalar('Loss/Train Loss', epoch_loss, epoch)
     tb_logger.add_scalar('Loss/Epoch Loss', epoch_loss, epoch)
     logging.info(f'\nEpoch: {epoch} | Train Loss: {epoch_loss:.4f} | Validation Loss: {val_loss:.4f}\n')
+
+    
     
 
 
 def tb_log_validation(tb_logger, optimizer, val_loss, acc, 
-    images, batch_size, global_step, epoch):
+    images, batch_size, global_step, epoch, net):
     # TensorBoard Storing the results
     tb_logger.add_scalar('Process/Learning Rate', optimizer.param_groups[0]['lr'], global_step)
     tb_logger.add_scalar('Loss/Validation Loss (Step)', val_loss, global_step)
@@ -64,13 +73,18 @@ def tb_log_validation(tb_logger, optimizer, val_loss, acc,
     # img_batch = images.cpu() if batch_size == 1 else [image.cpu() for image in images]
     # tb_logger.add_images('Data/Images', img_batch, global_step)
 
+    for name, param in net.named_parameters():
+        tb_logger.add_histogram(name, param.clone().cpu().data.numpy(), global_step)
+
+    
 
 
 if __name__ == '__main__':
     # Test
     tb_setup(dict(
+            net = MobileNet_V2(pretrained = True, image_channels = 1, num_classes = 229), 
             epochs = 1000, 
-            batch_size = 200, 
+            batch_size = 2, 
             learning_rate = 0.001,
             save_checkpoint = './checkpoints/', 
             amp = False,
diff --git a/utils/wandb_logger.py b/utils/wandb_logger.py
index eddf262..5b3b327 100644
--- a/utils/wandb_logger.py
+++ b/utils/wandb_logger.py
@@ -4,9 +4,6 @@ import wandb
 
 
 
-
-
-
 def wandb_setup(config) -> wandb:
     # Sign in to wandb
     wandb.login(key='0257777f14fecbf445207a8fdacdee681c72113a')
@@ -19,6 +16,7 @@ def wandb_setup(config) -> wandb:
     experiment = wandb.init(
         project = "Bone-Age-RSNA", 
         entity = "rsna-bone-age", 
+        sync_tensorboard = True,
         name = run_name, 
         tags = [
             'bone-age', 
@@ -27,6 +25,8 @@ def wandb_setup(config) -> wandb:
             f'{run_name}', 
             f'{device}'
         ],)
+    
+    # wandb.tensorboard.patch(root_logdir="./tensorboard", tensorboardX=True)
     # Configure wandb
     # experiment.config.update(dict(
     #     epochs = config['epochs'], 
@@ -46,10 +46,10 @@ def wandb_setup(config) -> wandb:
 def wandb_log_training_step(wandb_logger, loss, global_step, epoch, epoch_loss_step):
     # Logging
     wandb_logger.log({
-        'Loss/Step Loss':            loss.item(),
-        'Process/Step':                 global_step,
-        'Loss/Train Loss (Step)':    epoch_loss_step,
-        'Process/Epoch':                epoch
+        'wLoss/Step_Loss':            loss.item(),
+        'wProcess/Step':                 global_step,
+        'wLoss/Train_Loss_(Step)':    epoch_loss_step,
+        'wProcess/Epoch':                epoch
     })
 
 def wandb_log_training(wandb_logger, epoch_loss, val_loss, epoch):
