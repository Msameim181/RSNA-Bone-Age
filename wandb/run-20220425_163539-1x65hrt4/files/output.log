
INFO: WandB setup completed.
INFO: Training Settings:
        Device:             cuda
        Model:              MobileNetV2_Pre2
        Image Channel:      1
        Epochs:             10
        Batch Size:         2
        Learning Rate:      0.001
        Training Size:      8828
        validation Size:    3783
        validation %:       0.3
        Checkpoints:        True
        Mixed Precision:    False
        optimizer:          Adam
        criterion:          BCEWithLogitsLoss
INFO: Start training as "20220425_163536_MobileNetV2_Pre2" ...
























Epoch 1/10:   6%|██████▌                                                                                               | 568/8828 [00:53<12:52, 10.69img/s, Epoch Loss (Train)=0.0177, Step Loss (Batch)=0.0221]
Traceback (most recent call last):
  File "c:\Users\mahdi\Desktop\Github_Work\1\RSNA-Bone-Age\Run.py", line 86, in <module>
    trainer(
  File "c:\Users\mahdi\Desktop\Github_Work\1\RSNA-Bone-Age\Train.py", line 130, in trainer
    grad_scaler.step(optimizer)
  File "C:\Python\Python39\lib\site-packages\torch\cuda\amp\grad_scaler.py", line 310, in step
    return optimizer.step(*args, **kwargs)
  File "C:\Python\Python39\lib\site-packages\torch\optim\optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "C:\Python\Python39\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "C:\Python\Python39\lib\site-packages\torch\optim\adam.py", line 141, in step
    F.adam(params_with_grad,
  File "C:\Python\Python39\lib\site-packages\torch\optim\_functional.py", line 98, in adam
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)
KeyboardInterrupt