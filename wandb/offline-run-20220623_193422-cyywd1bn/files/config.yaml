wandb_version: 1

WandB_usage:
  desc: null
  value: true
_wandb:
  desc: null
  value:
    cli_version: 0.12.9
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    python_version: 3.8.5
    start_time: 1655996663
    t:
      1:
      - 1
      - 5
      2:
      - 1
      - 5
      3:
      - 4
      - 13
      - 15
      4: 3.8.5
      5: 0.12.9
      8:
      - 3
      - 5
amp:
  desc: null
  value: false
basedOnSex:
  desc: null
  value: false
batch_size:
  desc: null
  value: 20
criterion:
  desc: null
  value: MSELoss
dataset_name:
  desc: null
  value: rsna-bone-age-neu
device:
  desc: null
  value: cuda
epochs:
  desc: null
  value: 20
gender:
  desc: null
  value: male
learning_rate:
  desc: null
  value: 0.001
model:
  desc: null
  value: MobileNetV3_Pre
name:
  desc: null
  value: 20220623_193422_MobileNetV3_Pre_MSE_G-32_Transfer
net:
  desc: null
  value: "MobileNetV3(\n  (mobilenet_v3): MobileNetV3(\n    (features): Sequential(\n\
    \      (0): ConvNormActivation(\n        (0): Conv2d(1, 16, kernel_size=(3, 3),\
    \ stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=0.001,\
    \ momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n\
    \      )\n      (1): InvertedResidual(\n        (block): Sequential(\n       \
    \   (0): ConvNormActivation(\n            (0): Conv2d(16, 16, kernel_size=(3,\
    \ 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n            (1):\
    \ BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n\
    \            (2): ReLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n\
    \            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n\
    \            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n\
    \          )\n        )\n      )\n      (2): InvertedResidual(\n        (block):\
    \ Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(16,\
    \ 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(64,\
    \ eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n         \
    \   (2): ReLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n\
    \            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1,\
    \ 1), groups=64, bias=False)\n            (1): BatchNorm2d(64, eps=0.001, momentum=0.01,\
    \ affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n\
    \          )\n          (2): ConvNormActivation(\n            (0): Conv2d(64,\
    \ 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24,\
    \ eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n         \
    \ )\n        )\n      )\n      (3): InvertedResidual(\n        (block): Sequential(\n\
    \          (0): ConvNormActivation(\n            (0): Conv2d(24, 72, kernel_size=(1,\
    \ 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(72, eps=0.001,\
    \ momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n\
    \          )\n          (1): ConvNormActivation(\n            (0): Conv2d(72,\
    \ 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n\
    \            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n\
    \            (2): ReLU(inplace=True)\n          )\n          (2): ConvNormActivation(\n\
    \            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n\
    \            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n\
    \          )\n        )\n      )\n      (4): InvertedResidual(\n        (block):\
    \ Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(24,\
    \ 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(72,\
    \ eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n         \
    \   (2): ReLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n\
    \            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2,\
    \ 2), groups=72, bias=False)\n            (1): BatchNorm2d(72, eps=0.001, momentum=0.01,\
    \ affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n\
    \          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n\
    \            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n      \
    \      (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n            (activation):\
    \ ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n       \
    \   (3): ConvNormActivation(\n            (0): Conv2d(72, 40, kernel_size=(1,\
    \ 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=0.001,\
    \ momentum=0.01, affine=True, track_running_stats=True)\n          )\n       \
    \ )\n      )\n      (5): InvertedResidual(\n        (block): Sequential(\n   \
    \       (0): ConvNormActivation(\n            (0): Conv2d(40, 120, kernel_size=(1,\
    \ 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(120, eps=0.001,\
    \ momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n\
    \          )\n          (1): ConvNormActivation(\n            (0): Conv2d(120,\
    \ 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n\
    \            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n\
    \            (2): ReLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n\
    \            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(120,\
    \ 32, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(32, 120, kernel_size=(1,\
    \ 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation):\
    \ Hardsigmoid()\n          )\n          (3): ConvNormActivation(\n           \
    \ (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      \
    \      (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n\
    \          )\n        )\n      )\n      (6): InvertedResidual(\n        (block):\
    \ Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(40,\
    \ 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(120,\
    \ eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n         \
    \   (2): ReLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n\
    \            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2,\
    \ 2), groups=120, bias=False)\n            (1): BatchNorm2d(120, eps=0.001, momentum=0.01,\
    \ affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n\
    \          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n\
    \            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n     \
    \       (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n          \
    \  (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n     \
    \     )\n          (3): ConvNormActivation(\n            (0): Conv2d(120, 40,\
    \ kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40,\
    \ eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n         \
    \ )\n        )\n      )\n      (7): InvertedResidual(\n        (block): Sequential(\n\
    \          (0): ConvNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1,\
    \ 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=0.001,\
    \ momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n\
    \          )\n          (1): ConvNormActivation(\n            (0): Conv2d(240,\
    \ 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n\
    \            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n\
    \            (2): Hardswish()\n          )\n          (2): ConvNormActivation(\n\
    \            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n\
    \            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n\
    \          )\n        )\n      )\n      (8): InvertedResidual(\n        (block):\
    \ Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(80,\
    \ 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(200,\
    \ eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n         \
    \   (2): Hardswish()\n          )\n          (1): ConvNormActivation(\n      \
    \      (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),\
    \ groups=200, bias=False)\n            (1): BatchNorm2d(200, eps=0.001, momentum=0.01,\
    \ affine=True, track_running_stats=True)\n            (2): Hardswish()\n     \
    \     )\n          (2): ConvNormActivation(\n            (0): Conv2d(200, 80,\
    \ kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(80,\
    \ eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n         \
    \ )\n        )\n      )\n      (9): InvertedResidual(\n        (block): Sequential(\n\
    \          (0): ConvNormActivation(\n            (0): Conv2d(80, 184, kernel_size=(1,\
    \ 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(184, eps=0.001,\
    \ momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n\
    \          )\n          (1): ConvNormActivation(\n            (0): Conv2d(184,\
    \ 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n\
    \            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n\
    \            (2): Hardswish()\n          )\n          (2): ConvNormActivation(\n\
    \            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n\
    \            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n\
    \          )\n        )\n      )\n      (10): InvertedResidual(\n        (block):\
    \ Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(80,\
    \ 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(184,\
    \ eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n         \
    \   (2): Hardswish()\n          )\n          (1): ConvNormActivation(\n      \
    \      (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),\
    \ groups=184, bias=False)\n            (1): BatchNorm2d(184, eps=0.001, momentum=0.01,\
    \ affine=True, track_running_stats=True)\n            (2): Hardswish()\n     \
    \     )\n          (2): ConvNormActivation(\n            (0): Conv2d(184, 80,\
    \ kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(80,\
    \ eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n         \
    \ )\n        )\n      )\n      (11): InvertedResidual(\n        (block): Sequential(\n\
    \          (0): ConvNormActivation(\n            (0): Conv2d(80, 480, kernel_size=(1,\
    \ 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(480, eps=0.001,\
    \ momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n\
    \          )\n          (1): ConvNormActivation(\n            (0): Conv2d(480,\
    \ 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n\
    \            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n\
    \            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n\
    \            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(480,\
    \ 120, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(120, 480,\
    \ kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n     \
    \       (scale_activation): Hardsigmoid()\n          )\n          (3): ConvNormActivation(\n\
    \            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n\
    \            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n\
    \          )\n        )\n      )\n      (12): InvertedResidual(\n        (block):\
    \ Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(112,\
    \ 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672,\
    \ eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n         \
    \   (2): Hardswish()\n          )\n          (1): ConvNormActivation(\n      \
    \      (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),\
    \ groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=0.001, momentum=0.01,\
    \ affine=True, track_running_stats=True)\n            (2): Hardswish()\n     \
    \     )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n\
    \            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n    \
    \        (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n        \
    \    (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n   \
    \       )\n          (3): ConvNormActivation(\n            (0): Conv2d(672, 112,\
    \ kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112,\
    \ eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n         \
    \ )\n        )\n      )\n      (13): InvertedResidual(\n        (block): Sequential(\n\
    \          (0): ConvNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1,\
    \ 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=0.001,\
    \ momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n\
    \          )\n          (1): ConvNormActivation(\n            (0): Conv2d(672,\
    \ 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n\
    \            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n\
    \            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n\
    \            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672,\
    \ 168, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(168, 672,\
    \ kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n     \
    \       (scale_activation): Hardsigmoid()\n          )\n          (3): ConvNormActivation(\n\
    \            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n\
    \            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n\
    \          )\n        )\n      )\n      (14): InvertedResidual(\n        (block):\
    \ Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(160,\
    \ 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960,\
    \ eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n         \
    \   (2): Hardswish()\n          )\n          (1): ConvNormActivation(\n      \
    \      (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2),\
    \ groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=0.001, momentum=0.01,\
    \ affine=True, track_running_stats=True)\n            (2): Hardswish()\n     \
    \     )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n\
    \            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n    \
    \        (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n        \
    \    (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n   \
    \       )\n          (3): ConvNormActivation(\n            (0): Conv2d(960, 160,\
    \ kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(160,\
    \ eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n         \
    \ )\n        )\n      )\n      (15): InvertedResidual(\n        (block): Sequential(\n\
    \          (0): ConvNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1,\
    \ 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=0.001,\
    \ momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n\
    \          )\n          (1): ConvNormActivation(\n            (0): Conv2d(960,\
    \ 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n\
    \            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n\
    \            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n\
    \            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(960,\
    \ 240, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(240, 960,\
    \ kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n     \
    \       (scale_activation): Hardsigmoid()\n          )\n          (3): ConvNormActivation(\n\
    \            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n\
    \            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n\
    \          )\n        )\n      )\n      (16): ConvNormActivation(\n        (0):\
    \ Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1):\
    \ BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n\
    \        (2): Hardswish()\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=1)\n\
    \    (classifier): Sequential(\n      (0): Linear(in_features=992, out_features=2048,\
    \ bias=True)\n      (1): Hardswish()\n      (2): Dropout(p=0.2, inplace=False)\n\
    \      (3): Linear(in_features=2048, out_features=2048, bias=True)\n      (4):\
    \ Hardswish()\n      (5): Dropout(p=0.3, inplace=False)\n      (6): Linear(in_features=2048,\
    \ out_features=1024, bias=True)\n      (7): Hardswish()\n      (8): Dropout(p=0.3,\
    \ inplace=False)\n      (9): Linear(in_features=1024, out_features=512, bias=True)\n\
    \      (10): Hardswish()\n      (11): Linear(in_features=512, out_features=1,\
    \ bias=True)\n      (12): Sigmoid()\n    )\n    (fc_gender): Sequential(\n   \
    \   (0): Linear(in_features=1, out_features=32, bias=True)\n      (1): Hardswish()\n\
    \    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n)"
optimizer:
  desc: null
  value: Adam
save_checkpoint:
  desc: null
  value: true
test_dataset_size:
  desc: null
  value: 200
train_dataset_size:
  desc: null
  value: 12370
