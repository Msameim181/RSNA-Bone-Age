INFO: WandB setup completed.
INFO: Training Settings:
        Device:             cuda
        Model:              MobileNetV2_Pre2
        Image Channel:      1
        Epochs:             10
        Batch Size:         2
        Learning Rate:      0.001
        Training Size:      8828
        validation Size:    3783
        validation %:       0.3
        Checkpoints:        True
        Mixed Precision:    False
        optimizer:          Adam
        criterion:          BCEWithLogitsLoss
INFO: Start training as "20220425_150014_MobileNetV2_Pre2" ...



















































































































































































Epoch 1/10:  50%|█████████████████████████████████████████████████▌                                                 | 4414/8828 [06:07<05:54, 12.46img/s, Epoch Loss (Train)=0.0116, Step Loss (Batch)=0.0221]INFO: Val accept on 2207


























































































Validation set:
	Average loss: 0.0199	Accuracy: 11.29%	Correct = 427/3783
INFO: Validation completed.
INFO: Result Saved.







































































Epoch 1/10:  70%|█████████████████████████████████████████████████████████████████████▎                             | 6186/8828 [11:37<04:57,  8.87img/s, Epoch Loss (Train)=0.0112, Step Loss (Batch)=0.0229]
Traceback (most recent call last):
  File "c:\Users\mahdi\Desktop\Github_Work\1\RSNA-Bone-Age\Run.py", line 86, in <module>
    trainer(
  File "c:\Users\mahdi\Desktop\Github_Work\1\RSNA-Bone-Age\Train.py", line 129, in trainer
    grad_scaler.step(optimizer)
  File "C:\Python\Python39\lib\site-packages\torch\cuda\amp\grad_scaler.py", line 310, in step
    return optimizer.step(*args, **kwargs)
  File "C:\Python\Python39\lib\site-packages\torch\optim\optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "C:\Python\Python39\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "C:\Python\Python39\lib\site-packages\torch\optim\adam.py", line 141, in step
    F.adam(params_with_grad,
  File "C:\Python\Python39\lib\site-packages\torch\optim\_functional.py", line 105, in adam
    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
KeyboardInterrupt